{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.transformer import DecoderTransformer\n",
    "from src.data import random_values, create_signals, sine, cosine_squared, create_batches\n",
    "from src.train_eval import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68781f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "TIME_STEP = 0.5 / MAX_SEQUENCE_LENGTH\n",
    "MIN_OMEGA = 85\n",
    "MAX_OMEGA = 115\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_COUNT = 64_000\n",
    "EPOCHS = 10\n",
    "\n",
    "assert 1 / TIME_STEP >= 2 * MAX_OMEGA, \"sampling rate not high enough\"\n",
    "assert TRAIN_COUNT % BATCH_SIZE == 0, \"batch size should divide train count\"\n",
    "\n",
    "transformer = DecoderTransformer(\n",
    "    output_parameter_count=1,\n",
    "    d_model=128,\n",
    "    num_heads=16,\n",
    "    num_layers=2,\n",
    "    d_ff=512,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_100 = torch.ones(TRAIN_COUNT) * 100\n",
    "for i in range(EPOCHS):\n",
    "    # generate new data - can be anything - sine waves, more complicated waves, combinations with different phases / frequencies\n",
    "    frequencies = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n",
    "    phases = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n",
    "        \n",
    "    sines = create_signals(\n",
    "        omegas=frequencies,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases\n",
    "    )\n",
    "    \n",
    "    cosines = create_signals(\n",
    "        omegas=only_100,\n",
    "        signal_function=cosine_squared,\n",
    "        length=MAX_SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases\n",
    "    )\n",
    "    \n",
    "    values = 0.7 * sines + 0.3 * cosines\n",
    "    batched_values, batched_parameters = create_batches([values, frequencies.reshape(-1, 1)], BATCH_SIZE)\n",
    "\n",
    "    # train an epoch\n",
    "    train_one_epoch(transformer, optim, criterion, device, batched_values, batched_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c98548",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n",
    "phases = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n",
    "        \n",
    "sines = create_signals(\n",
    "    omegas=frequencies,\n",
    "    signal_function=sine,\n",
    "    length=MAX_SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    "    phases=phases\n",
    ")\n",
    "    \n",
    "cosines = create_signals(\n",
    "    omegas=only_100,\n",
    "    signal_function=cosine_squared,\n",
    "    length=MAX_SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    "    phases=phases\n",
    ")\n",
    "    \n",
    "values = 0.7 * sines + 0.3 * cosines\n",
    "batched_values, batched_parameters = create_batches([values, frequencies.reshape(-1, 1)], BATCH_SIZE)\n",
    "\n",
    "eval_loss = evaluate(transformer, criterion, device, batched_values, batched_parameters)\n",
    "print(f\"Evaluation loss: {eval_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
