{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4e387b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-27T14:11:34.643215Z",
     "iopub.status.busy": "2025-12-27T14:11:34.642908Z",
     "iopub.status.idle": "2025-12-27T14:11:39.968338Z",
     "shell.execute_reply": "2025-12-27T14:11:39.967650Z"
    },
    "papermill": {
     "duration": 5.331323,
     "end_time": "2025-12-27T14:11:39.970386",
     "exception": false,
     "start_time": "2025-12-27T14:11:34.639063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from math import log, exp\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_mask(tgt):\n",
    "    seq_length = tgt.size(1)\n",
    "    no_peek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n",
    "    return no_peek_mask\n",
    "\n",
    "\n",
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, output_parameter_count, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, output_parameter_count)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_embedded = self.dropout(self.positional_encoding(src.reshape(-1, self.max_seq_length, 1)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "\n",
    "        output = self.fc(enc_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "def train_one_epoch(model: nn.Module, optimizer: torch.optim.Optimizer, loss_function, device, batched_samples: torch.Tensor, batched_params: torch.Tensor):\n",
    "    model.train()\n",
    "\n",
    "    for samples, params in zip(batched_samples, batched_params):\n",
    "        samples = samples.to(device)\n",
    "        params = params.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(samples)\n",
    "        loss = loss_function(predictions, params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train(model: nn.Module, optimizer: torch.optim.Optimizer, loss_function, device, batched_samples: torch.Tensor, batched_params: torch.Tensor, epochs: int, eval_step: int | None = None):\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(model, optimizer, loss_function, device, batched_samples, batched_params)\n",
    "        if eval_step is not None and (epoch + 1) % eval_step == 0:\n",
    "            evaluation = evaluate(model, loss_function, batched_samples, batched_params)\n",
    "            print(f\"Epoch {epoch + 1}: {evaluation} after {datetime.datetime.now() - start}\")\n",
    "\n",
    "def evaluate(model: nn.Module, loss_function, batched_eval_samples: torch.Tensor, batched_eval_params: torch.Tensor):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for samples, params in zip(batched_eval_samples, batched_eval_params):\n",
    "        samples = samples.to(device)\n",
    "        params = params.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(samples)\n",
    "            loss = loss_function(predictions, params).item()\n",
    "            total_loss += loss\n",
    "\n",
    "    return total_loss / (batched_eval_samples.size(0) * batched_eval_samples.size(1))\n",
    "\n",
    "def predict_next_values(model: nn.Module, values: torch.Tensor, sequence_length: int, count: int):\n",
    "    assert values.size(0) == sequence_length, f\"length of values should be {sequence_length}\"\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    values = values.to(device)\n",
    "    \n",
    "    all_values = torch.empty(sequence_length + count, device=device)\n",
    "    all_values[:sequence_length] = values\n",
    "\n",
    "    for i in range(count):\n",
    "        current_values = all_values[i:i+sequence_length]\n",
    "        with torch.no_grad():\n",
    "            predicted_next_value = model(current_values.reshape(1, -1)).item()\n",
    "        all_values[i + sequence_length] = predicted_next_value\n",
    "\n",
    "    return all_values.cpu().split((sequence_length, count))\n",
    "\n",
    "def random_values(count: int, minimum, maximum):\n",
    "    return torch.rand(count) * (maximum - minimum) + minimum\n",
    "\n",
    "def create_signals(omegas: torch.Tensor, signal_function, length: int, time_step: float, phases: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    count = omegas.size(0)\n",
    "    waves = torch.empty(count, length)\n",
    "\n",
    "    if phases is None:\n",
    "        phases = torch.zeros(count)\n",
    "\n",
    "    times = torch.arange(0, length) * time_step\n",
    "    for i in range(count):\n",
    "        wave = times * omegas[i] + phases[i]\n",
    "        waves[i] = signal_function(wave)\n",
    "    \n",
    "    return waves\n",
    "\n",
    "def sine(inputs: torch.Tensor):\n",
    "    return inputs.sin()\n",
    "\n",
    "def cosine_squared(inputs: torch.Tensor):\n",
    "    return inputs.cos().square()\n",
    "\n",
    "def sine_plus_cosine_squared(cosine_weight: float):\n",
    "    def tensor_function(inputs: torch.Tensor):\n",
    "        return sine(inputs) + cosine_weight * cosine_squared(inputs)\n",
    "    \n",
    "    return tensor_function\n",
    "\n",
    "def lengthen_tensors(first: torch.Tensor, second: torch.Tensor) -> tuple[np.ndarray, np.ndarray]:\n",
    "    first_length = first.size(0)\n",
    "    second_length = second.size(0)\n",
    "\n",
    "    first_lengthened = np.empty(first_length + second_length)\n",
    "    first_lengthened[:first_length] = first\n",
    "    first_lengthened[first_length:] = np.nan\n",
    "\n",
    "    second_lengthened = np.empty(first_length + second_length)\n",
    "    second_lengthened[:first_length] = np.nan\n",
    "    second_lengthened[first_length:] = second\n",
    "\n",
    "    return first_lengthened, second_lengthened\n",
    "\n",
    "def plot_prediction(values: torch.Tensor, predicted_values: torch.Tensor, time_step: float):\n",
    "    first, second = lengthen_tensors(values, predicted_values)\n",
    "    time = np.arange(first.shape[0]) * time_step\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(time, first)\n",
    "    ax.plot(time, second)\n",
    "    return fig\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def create_batches(unbatched_list: list[torch.Tensor], batch_size: int):\n",
    "    batch_count = unbatched_list[0].size(0) // batch_size\n",
    "    result = []\n",
    "\n",
    "    for unbatched in unbatched_list:\n",
    "        result.append(unbatched.reshape(batch_count, batch_size, *unbatched.shape[1:]))\n",
    "    \n",
    "    return tuple(result)\n",
    "\n",
    "def add_noise(values: torch.Tensor, noise_strength: float):\n",
    "    noise = noise_strength * torch.randn_like(values)\n",
    "    return values + noise\n",
    "\n",
    "def remove_samples(values: torch.Tensor, missing_segment_length: int, missing_segment_count: int):\n",
    "    sample_count = values.size(0)\n",
    "    sample_length = values.size(1)\n",
    "    new_values = values.detach().clone()\n",
    "\n",
    "    random_indices = torch.randint(sample_length - missing_segment_length, (sample_count, missing_segment_count))\n",
    "\n",
    "    for i in range(sample_count):\n",
    "        for j in range(missing_segment_count):\n",
    "            random_index = random_indices[i, j]\n",
    "            new_values[i, random_index:random_index+missing_segment_length] = -1e9\n",
    "    \n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc6dc5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T14:11:39.975918Z",
     "iopub.status.busy": "2025-12-27T14:11:39.975208Z",
     "iopub.status.idle": "2025-12-27T14:11:44.812643Z",
     "shell.execute_reply": "2025-12-27T14:11:44.811909Z"
    },
    "papermill": {
     "duration": 4.841834,
     "end_time": "2025-12-27T14:11:44.814191",
     "exception": false,
     "start_time": "2025-12-27T14:11:39.972357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1712896"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "def create_model(device: str):\n",
    "    return EncoderTransformer(\n",
    "        output_parameter_count=MAX_SEQUENCE_LENGTH,\n",
    "        d_model=256,\n",
    "        num_heads=16,\n",
    "        num_layers=3,\n",
    "        d_ff=512,\n",
    "        max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "        dropout=0\n",
    "    ).to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "transformer = create_model(device)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "count_parameters(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7a5bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T14:11:44.818781Z",
     "iopub.status.busy": "2025-12-27T14:11:44.818414Z",
     "iopub.status.idle": "2025-12-27T14:11:44.824334Z",
     "shell.execute_reply": "2025-12-27T14:11:44.823766Z"
    },
    "papermill": {
     "duration": 0.009754,
     "end_time": "2025-12-27T14:11:44.825634",
     "exception": false,
     "start_time": "2025-12-27T14:11:44.815880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIME_STEP = 0.5 / MAX_SEQUENCE_LENGTH\n",
    "MIN_OMEGA = 80\n",
    "MAX_OMEGA = 120\n",
    "HOLE_WIDTH = 50\n",
    "HOLE_COUNT = 2\n",
    "NOISE = 0.025\n",
    "\n",
    "def create_data(count: int):\n",
    "    frequencies_1 = random_values(count, MIN_OMEGA, MAX_OMEGA)\n",
    "    frequencies_2 = random_values(count, MIN_OMEGA, MAX_OMEGA)\n",
    "    frequencies_3 = random_values(count, MIN_OMEGA, MAX_OMEGA)\n",
    "    \n",
    "    phases_1 = random_values(count, 0, 2 * torch.pi)\n",
    "    phases_2 = random_values(count, 0, 2 * torch.pi)\n",
    "    phases_3 = random_values(count, 0, 2 * torch.pi)\n",
    "            \n",
    "    sine_1 = create_signals(\n",
    "        omegas=frequencies_1,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_1\n",
    "    )\n",
    "\n",
    "    sine_2 = create_signals(\n",
    "        omegas=frequencies_2,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_2\n",
    "    )\n",
    "\n",
    "    sine_3 = create_signals(\n",
    "        omegas=frequencies_3,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_3\n",
    "    )\n",
    "\n",
    "    signals = (sine_1 + sine_2 + sine_3) / 3\n",
    "    messy_signals = add_noise(signals, NOISE)\n",
    "    messy_signals = remove_samples(messy_signals, HOLE_WIDTH, HOLE_COUNT)\n",
    "    return messy_signals, signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c1a112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T14:11:44.829991Z",
     "iopub.status.busy": "2025-12-27T14:11:44.829445Z",
     "iopub.status.idle": "2025-12-27T18:12:27.884745Z",
     "shell.execute_reply": "2025-12-27T18:12:27.883976Z"
    },
    "papermill": {
     "duration": 14443.062148,
     "end_time": "2025-12-27T18:12:27.889299",
     "exception": false,
     "start_time": "2025-12-27T14:11:44.827151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished\n",
      "Epoch 2 finished\n",
      "Epoch 3 finished\n",
      "Epoch 4 finished\n",
      "Epoch 5 finished\n",
      "Epoch 6 finished\n",
      "Epoch 7 finished\n",
      "Epoch 8 finished\n",
      "Epoch 9 finished\n",
      "Epoch 10 finished\n",
      "Epoch 11 finished\n",
      "Epoch 12 finished\n",
      "Epoch 13 finished\n",
      "Epoch 14 finished\n",
      "Epoch 15 finished\n",
      "Epoch 16 finished\n",
      "Epoch 17 finished\n",
      "Epoch 18 finished\n",
      "Epoch 19 finished\n",
      "Epoch 20 finished\n",
      "Epoch 21 finished\n",
      "Epoch 22 finished\n",
      "Epoch 23 finished\n",
      "Epoch 24 finished\n",
      "Epoch 25 finished\n",
      "Epoch 26 finished\n",
      "Epoch 27 finished\n",
      "Epoch 28 finished\n",
      "Epoch 29 finished\n",
      "Epoch 30 finished\n",
      "Epoch 31 finished\n",
      "Epoch 32 finished\n",
      "Epoch 33 finished\n",
      "Epoch 34 finished\n",
      "Epoch 35 finished\n",
      "Epoch 36 finished\n",
      "Epoch 37 finished\n",
      "Epoch 38 finished\n",
      "Epoch 39 finished\n",
      "Epoch 40 finished\n",
      "Epoch 41 finished\n",
      "Epoch 42 finished\n",
      "Epoch 43 finished\n",
      "Epoch 44 finished\n",
      "Epoch 45 finished\n",
      "Epoch 46 finished\n",
      "Epoch 47 finished\n",
      "Epoch 48 finished\n",
      "Epoch 49 finished\n",
      "Epoch 50 finished\n",
      "Epoch 51 finished\n",
      "Epoch 52 finished\n",
      "Epoch 53 finished\n",
      "Epoch 54 finished\n",
      "Epoch 55 finished\n",
      "Epoch 56 finished\n",
      "Epoch 57 finished\n",
      "Epoch 58 finished\n",
      "Epoch 59 finished\n",
      "Epoch 60 finished\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 60\n",
    "TRAIN_COUNT = 64_000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    values, parameters = create_data(TRAIN_COUNT)\n",
    "    batched_values, batched_parameters = create_batches([values, parameters], BATCH_SIZE)\n",
    "    train_one_epoch(transformer, optim, criterion, device, batched_values, batched_parameters)\n",
    "    torch.save(transformer.state_dict(), f\"{i+1}.pt\")\n",
    "    print(f\"Epoch {i+1} finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6af9fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T18:12:27.897573Z",
     "iopub.status.busy": "2025-12-27T18:12:27.897099Z",
     "iopub.status.idle": "2025-12-27T18:20:13.712639Z",
     "shell.execute_reply": "2025-12-27T18:20:13.711827Z"
    },
    "papermill": {
     "duration": 465.82622,
     "end_time": "2025-12-27T18:20:13.718964",
     "exception": false,
     "start_time": "2025-12-27T18:12:27.892744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 79.21369369506836\n",
      "Epoch 2: 14.727992687225342\n",
      "Epoch 3: 13.475112857818603\n",
      "Epoch 4: 10.661662921905517\n",
      "Epoch 5: 9.200533695220948\n",
      "Epoch 6: 8.218801498413086\n",
      "Epoch 7: 7.386648387908935\n",
      "Epoch 8: 6.856926965713501\n",
      "Epoch 9: 6.10939736366272\n",
      "Epoch 10: 5.694736909866333\n",
      "Epoch 11: 5.25014513015747\n",
      "Epoch 12: 4.921695713996887\n",
      "Epoch 13: 4.465054459571839\n",
      "Epoch 14: 4.0948410320281985\n",
      "Epoch 15: 3.930643048286438\n",
      "Epoch 16: 3.7203274631500243\n",
      "Epoch 17: 3.3511900520324707\n",
      "Epoch 18: 3.354081754684448\n",
      "Epoch 19: 3.0760944747924803\n",
      "Epoch 20: 2.97194571018219\n",
      "Epoch 21: 2.7260362100601196\n",
      "Epoch 22: 2.7847671365737914\n",
      "Epoch 23: 2.811449580192566\n",
      "Epoch 24: 2.591637215614319\n",
      "Epoch 25: 2.3846767020225523\n",
      "Epoch 26: 2.39386549949646\n",
      "Epoch 27: 2.198474397659302\n",
      "Epoch 28: 2.170709316730499\n",
      "Epoch 29: 2.0648192739486695\n",
      "Epoch 30: 2.6998743295669554\n",
      "Epoch 31: 2.1910663652420044\n",
      "Epoch 32: 2.0210211205482485\n",
      "Epoch 33: 2.0074137353897097\n",
      "Epoch 34: 2.02333377122879\n",
      "Epoch 35: 2.281569519042969\n",
      "Epoch 36: 2.357478768825531\n",
      "Epoch 37: 2.250084414482117\n",
      "Epoch 38: 2.1290335202217103\n",
      "Epoch 39: 1.9077901530265808\n",
      "Epoch 40: 2.132135784626007\n",
      "Epoch 41: 1.8394398593902588\n",
      "Epoch 42: 1.863539218902588\n",
      "Epoch 43: 2.5421384811401366\n",
      "Epoch 44: 2.4725777292251587\n",
      "Epoch 45: 2.671843490600586\n",
      "Epoch 46: 2.022693328857422\n",
      "Epoch 47: 1.8061230731010438\n",
      "Epoch 48: 1.8985402989387512\n",
      "Epoch 49: 1.6860455346107484\n",
      "Epoch 50: 2.142549297809601\n",
      "Epoch 51: 3.0643400049209593\n",
      "Epoch 52: 3.16041042804718\n",
      "Epoch 53: 2.5621683597564697\n",
      "Epoch 54: 2.1446658611297607\n",
      "Epoch 55: 4.906438255310059\n",
      "Epoch 56: 3.940617504119873\n",
      "Epoch 57: 2.890481095314026\n",
      "Epoch 58: 2.520065732002258\n",
      "Epoch 59: 2.6179830598831177\n",
      "Epoch 60: 2.204975688457489\n"
     ]
    }
   ],
   "source": [
    "EVAL_COUNT = 6_400\n",
    "\n",
    "eval_values, eval_parameters = create_data(EVAL_COUNT)\n",
    "eval_values_batched, eval_parameters_batched = create_batches([eval_values, eval_parameters], BATCH_SIZE)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model = create_model(device)\n",
    "    model.load_state_dict(torch.load(f\"{i+1}.pt\"))\n",
    "    evaluation = evaluate(model, criterion, eval_values_batched, eval_parameters_batched)\n",
    "    print(f\"Epoch {i + 1}: {evaluation}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 539775,
     "modelInstanceId": 525714,
     "sourceId": 693297,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14923.362227,
   "end_time": "2025-12-27T18:20:15.546262",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-27T14:11:32.184035",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
