{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nfrom math import log, exp\nimport math\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        # Ensure that the model dimension (d_model) is divisible by the number of heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        # Initialize dimensions\n        self.d_model = d_model # Model's dimension\n        self.num_heads = num_heads # Number of attention heads\n        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n\n        # Linear layers for transforming inputs\n        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Calculate attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax is applied to obtain attention probabilities\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n\n        # Multiply by values to obtain the final output\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def split_heads(self, x):\n        # Reshape the input to have num_heads for multi-head attention\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        # Combine the multiple heads back to original shape\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        # Apply linear transformations and split heads\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n\n        # Perform scaled dot-product attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n\n        # Combine heads and apply output transformation\n        output = self.W_o(self.combine_heads(attn_output))\n        return output\n\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x\n\n\ndef generate_mask(tgt):\n    seq_length = tgt.size(1)\n    no_peek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n    return no_peek_mask\n\n\nclass DecoderTransformer(nn.Module):\n    def __init__(self, output_parameter_count, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(DecoderTransformer, self).__init__()\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.fc = nn.Linear(d_model, output_parameter_count)\n        self.dropout = nn.Dropout(dropout)\n        self.max_seq_length = max_seq_length\n\n    def forward(self, tgt):\n        tgt_mask = generate_mask(tgt)\n        tgt_embedded = self.dropout(self.positional_encoding(tgt.reshape(-1, self.max_seq_length, 1)))\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, tgt_mask)\n\n        output = self.fc(dec_output[:, -1, :])\n        return output\n\ndef train_one_epoch(model: nn.Module, optimizer: torch.optim.Optimizer, loss_function, device, batched_samples: torch.Tensor, batched_params: torch.Tensor):\n    model.train()\n\n    for samples, params in zip(batched_samples, batched_params):\n        samples = samples.to(device)\n        params = params.to(device)\n        \n        optimizer.zero_grad()\n        predictions = model(samples)\n        loss = loss_function(predictions, params)\n        loss.backward()\n        optimizer.step()\n\ndef evaluate(model: nn.Module, loss_function, batched_eval_samples: torch.Tensor, batched_eval_params: torch.Tensor):\n    model.eval()\n\n    total_loss = 0.0\n    for samples, params in zip(batched_eval_samples, batched_eval_params):\n        samples = samples.to(device)\n        params = params.to(device)\n        \n        with torch.no_grad():\n            predictions = model(samples)\n            loss = loss_function(predictions, params).item()\n            total_loss += loss\n\n    return total_loss / (batched_eval_samples.size(0) * batched_eval_samples.size(1))\n\ndef random_values(count: int, minimum, maximum):\n    return torch.rand(count) * (maximum - minimum) + minimum\n\ndef create_signals(omegas: torch.Tensor, signal_function, length: int, time_step: float, phases: torch.Tensor | None = None) -> torch.Tensor:\n    count = omegas.size(0)\n    waves = torch.empty(count, length)\n\n    if phases is None:\n        phases = torch.zeros(count)\n\n    times = torch.arange(0, length) * time_step\n    for i in range(count):\n        wave = times * omegas[i] + phases[i]\n        waves[i] = signal_function(wave)\n    \n    return waves\n\ndef sine(inputs: torch.Tensor):\n    return inputs.sin()\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_batches(unbatched_list: list[torch.Tensor], batch_size: int):\n    batch_count = unbatched_list[0].size(0) // batch_size\n    result = []\n\n    for unbatched in unbatched_list:\n        result.append(unbatched.reshape(batch_count, batch_size, *unbatched.shape[1:]))\n    \n    return tuple(result)\n\ndef add_noise(values: torch.Tensor, noise_strength: float):\n    noise = noise_strength * torch.randn_like(values)\n    return values + noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:40:46.925764Z","iopub.execute_input":"2025-12-15T19:40:46.926483Z","iopub.status.idle":"2025-12-15T19:40:46.957072Z","shell.execute_reply.started":"2025-12-15T19:40:46.926452Z","shell.execute_reply":"2025-12-15T19:40:46.956476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nMAX_SEQUENCE_LENGTH = 512\n\ndef make_model():\n    return DecoderTransformer(\n        output_parameter_count=1,\n        d_model=256,\n        num_heads=16,\n        num_layers=3,\n        d_ff=512,\n        max_seq_length=MAX_SEQUENCE_LENGTH,\n        dropout=0.2\n    ).to(device)\n\ntransformer = make_model()\noptim = torch.optim.Adam(transformer.parameters(), lr=0.0001)\ncriterion = nn.MSELoss(reduction=\"sum\")\ncount_parameters(transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:40:49.294203Z","iopub.execute_input":"2025-12-15T19:40:49.294483Z","iopub.status.idle":"2025-12-15T19:40:49.323212Z","shell.execute_reply.started":"2025-12-15T19:40:49.294461Z","shell.execute_reply":"2025-12-15T19:40:49.322630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TIME_STEP = 0.5 / MAX_SEQUENCE_LENGTH\nMIN_OMEGA = 80\nMAX_OMEGA = 120\nBATCH_SIZE = 128\nTRAIN_COUNT = 64_000\nNOISE = 0.025\n\ndef create_data():\n    assert 1 / TIME_STEP >= 2 * MAX_OMEGA, \"sampling rate not high enough\"\n    assert TRAIN_COUNT % BATCH_SIZE == 0, \"batch size should divide train count\"\n\n    frequencies_1 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n    frequencies_2 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n    frequencies_3 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n    \n    phases_1 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n    phases_2 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n    phases_3 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n            \n    sine_1 = create_signals(\n        omegas=frequencies_1,\n        signal_function=sine,\n        length=MAX_SEQUENCE_LENGTH + 1,\n        time_step=TIME_STEP,\n        phases=phases_1\n    )\n        \n    sine_2 = create_signals(\n        omegas=frequencies_2,\n        signal_function=sine,\n        length=MAX_SEQUENCE_LENGTH + 1,\n        time_step=TIME_STEP,\n        phases=phases_2\n    )\n\n    sine_3 = create_signals(\n        omegas=frequencies_3,\n        signal_function=sine,\n        length=MAX_SEQUENCE_LENGTH + 1,\n        time_step=TIME_STEP,\n        phases=phases_3\n    )\n    \n    signals = (sine_1 + sine_2 + sine_3) / 3\n    values, next_value = signals.split((MAX_SEQUENCE_LENGTH, 1), dim=1)\n    values = add_noise(values, NOISE)\n    return create_batches([values, next_value], BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:40:51.440440Z","iopub.execute_input":"2025-12-15T19:40:51.440709Z","iopub.status.idle":"2025-12-15T19:40:51.446998Z","shell.execute_reply.started":"2025-12-15T19:40:51.440690Z","shell.execute_reply":"2025-12-15T19:40:51.446328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 50\n\nstart = datetime.datetime.now()\nfor i in range(EPOCHS):\n    batched_values, batched_parameters = create_data()\n    train_one_epoch(transformer, optim, criterion, device, batched_values, batched_parameters)\n    torch.save(transformer.state_dict(), f\"{i + 1}.pt\")\n    print(i + 1, \"after\", datetime.datetime.now() - start)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:40:53.980354Z","iopub.execute_input":"2025-12-15T19:40:53.981079Z","iopub.status.idle":"2025-12-15T19:45:43.117555Z","shell.execute_reply.started":"2025-12-15T19:40:53.981054Z","shell.execute_reply":"2025-12-15T19:45:43.116897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_x, eval_y = create_data()\nfor i in range(EPOCHS):\n    model = make_model()\n    model.load_state_dict(torch.load(f\"{i + 1}.pt\"))\n    loss = evaluate(model, criterion, eval_x, eval_y)\n    print(f\"Epoch {i + 1}: {loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:48:24.227511Z","iopub.execute_input":"2025-12-15T19:48:24.228172Z","iopub.status.idle":"2025-12-15T19:50:06.191692Z","shell.execute_reply.started":"2025-12-15T19:48:24.228151Z","shell.execute_reply":"2025-12-15T19:50:06.190863Z"}},"outputs":[],"execution_count":null}]}