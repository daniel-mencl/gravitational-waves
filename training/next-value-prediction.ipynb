{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:40:46.926483Z",
     "iopub.status.busy": "2025-12-15T19:40:46.925764Z",
     "iopub.status.idle": "2025-12-15T19:40:46.957072Z",
     "shell.execute_reply": "2025-12-15T19:40:46.956476Z",
     "shell.execute_reply.started": "2025-12-15T19:40:46.926452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from math import log, exp\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_mask(tgt):\n",
    "    seq_length = tgt.size(1)\n",
    "    no_peek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n",
    "    return no_peek_mask\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, output_parameter_count, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, output_parameter_count)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_mask = generate_mask(tgt)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(tgt.reshape(-1, self.max_seq_length, 1)))\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "def train_one_epoch(model: nn.Module, optimizer: torch.optim.Optimizer, loss_function, device, batched_samples: torch.Tensor, batched_params: torch.Tensor):\n",
    "    model.train()\n",
    "\n",
    "    for samples, params in zip(batched_samples, batched_params):\n",
    "        samples = samples.to(device)\n",
    "        params = params.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(samples)\n",
    "        loss = loss_function(predictions, params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model: nn.Module, loss_function, device, batched_eval_samples: torch.Tensor, batched_eval_params: torch.Tensor):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for samples, params in zip(batched_eval_samples, batched_eval_params):\n",
    "        samples = samples.to(device)\n",
    "        params = params.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(samples)\n",
    "            loss = loss_function(predictions, params).item()\n",
    "            total_loss += loss\n",
    "\n",
    "    return total_loss / (batched_eval_samples.size(0) * batched_eval_samples.size(1))\n",
    "\n",
    "def random_values(count: int, minimum, maximum):\n",
    "    return torch.rand(count) * (maximum - minimum) + minimum\n",
    "\n",
    "def create_signals(omegas: torch.Tensor, signal_function, length: int, time_step: float, phases: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    count = omegas.size(0)\n",
    "    waves = torch.empty(count, length)\n",
    "\n",
    "    if phases is None:\n",
    "        phases = torch.zeros(count)\n",
    "\n",
    "    times = torch.arange(0, length) * time_step\n",
    "    for i in range(count):\n",
    "        wave = times * omegas[i] + phases[i]\n",
    "        waves[i] = signal_function(wave)\n",
    "    \n",
    "    return waves\n",
    "\n",
    "def sine(inputs: torch.Tensor):\n",
    "    return inputs.sin()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def create_batches(unbatched_list: list[torch.Tensor], batch_size: int):\n",
    "    batch_count = unbatched_list[0].size(0) // batch_size\n",
    "    result = []\n",
    "\n",
    "    for unbatched in unbatched_list:\n",
    "        result.append(unbatched.reshape(batch_count, batch_size, *unbatched.shape[1:]))\n",
    "    \n",
    "    return tuple(result)\n",
    "\n",
    "def add_noise(values: torch.Tensor, noise_strength: float):\n",
    "    noise = noise_strength * torch.randn_like(values)\n",
    "    return values + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:40:49.294483Z",
     "iopub.status.busy": "2025-12-15T19:40:49.294203Z",
     "iopub.status.idle": "2025-12-15T19:40:49.323212Z",
     "shell.execute_reply": "2025-12-15T19:40:49.322630Z",
     "shell.execute_reply.started": "2025-12-15T19:40:49.294461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1581569"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "def make_model():\n",
    "    return DecoderTransformer(\n",
    "        output_parameter_count=1,\n",
    "        d_model=256,\n",
    "        num_heads=16,\n",
    "        num_layers=3,\n",
    "        d_ff=512,\n",
    "        max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "transformer = make_model()\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "count_parameters(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:40:51.440709Z",
     "iopub.status.busy": "2025-12-15T19:40:51.440440Z",
     "iopub.status.idle": "2025-12-15T19:40:51.446998Z",
     "shell.execute_reply": "2025-12-15T19:40:51.446328Z",
     "shell.execute_reply.started": "2025-12-15T19:40:51.440690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TIME_STEP = 0.5 / MAX_SEQUENCE_LENGTH\n",
    "MIN_OMEGA = 80\n",
    "MAX_OMEGA = 120\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_COUNT = 64_000\n",
    "NOISE = 0.025\n",
    "\n",
    "def create_data():\n",
    "    assert 1 / TIME_STEP >= 2 * MAX_OMEGA, \"sampling rate not high enough\"\n",
    "    assert TRAIN_COUNT % BATCH_SIZE == 0, \"batch size should divide train count\"\n",
    "\n",
    "    frequencies_1 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n",
    "    frequencies_2 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n",
    "    frequencies_3 = random_values(TRAIN_COUNT, MIN_OMEGA, MAX_OMEGA)\n",
    "    \n",
    "    phases_1 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n",
    "    phases_2 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n",
    "    phases_3 = random_values(TRAIN_COUNT, 0, 2 * torch.pi)\n",
    "            \n",
    "    sine_1 = create_signals(\n",
    "        omegas=frequencies_1,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_1\n",
    "    )\n",
    "        \n",
    "    sine_2 = create_signals(\n",
    "        omegas=frequencies_2,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_2\n",
    "    )\n",
    "\n",
    "    sine_3 = create_signals(\n",
    "        omegas=frequencies_3,\n",
    "        signal_function=sine,\n",
    "        length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        time_step=TIME_STEP,\n",
    "        phases=phases_3\n",
    "    )\n",
    "    \n",
    "    signals = (sine_1 + sine_2 + sine_3) / 3\n",
    "    values, next_value = signals.split((MAX_SEQUENCE_LENGTH, 1), dim=1)\n",
    "    values = add_noise(values, NOISE)\n",
    "    return create_batches([values, next_value], BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:40:53.981079Z",
     "iopub.status.busy": "2025-12-15T19:40:53.980354Z",
     "iopub.status.idle": "2025-12-15T19:45:43.117555Z",
     "shell.execute_reply": "2025-12-15T19:45:43.116897Z",
     "shell.execute_reply.started": "2025-12-15T19:40:53.981054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "for i in range(EPOCHS):\n",
    "    batched_values, batched_parameters = create_data()\n",
    "    train_one_epoch(transformer, optim, criterion, device, batched_values, batched_parameters)\n",
    "    torch.save(transformer.state_dict(), f\"{i + 1}.pt\")\n",
    "    print(i + 1, \"after\", datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:48:24.228172Z",
     "iopub.status.busy": "2025-12-15T19:48:24.227511Z",
     "iopub.status.idle": "2025-12-15T19:50:06.191692Z",
     "shell.execute_reply": "2025-12-15T19:50:06.190863Z",
     "shell.execute_reply.started": "2025-12-15T19:48:24.228151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_x, eval_y = create_data()\n",
    "for i in range(EPOCHS):\n",
    "    model = make_model()\n",
    "    model.load_state_dict(torch.load(f\"{i + 1}.pt\"))\n",
    "    loss = evaluate(model, criterion, eval_x, eval_y)\n",
    "    print(f\"Epoch {i + 1}: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
